{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatProgCsom nyolcadik gyakorlat - machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az sklearn csomagban nagyon sok klasszikus modszer es minden hozzajuk kotodo fuggveny le van implementalva.\n",
    "\n",
    "Telepites:\n",
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. feladat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenerÃ¡ljunk ket, 2D standard normalis eloszlasbol szarmazo adathalmazt. A varhato ertekeket es kovarianciakat varja a fuggveny parameterkent es olyan ertekeket adjunk meg, amikre linearisan szeparalhato lesz a ket adathalmaz. Plotoljuk is ki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. feladat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irjuk meg a Perceptron algoritmust, lepesenkent plotoljuk a valtozast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. feladat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keressuk meg az sklearnben a Perceptron algoritmust es hasonlitsuk ossze a sajatunkkal a vegeredmenyt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. feladat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fent generalt adaton tanitsunk linearis SVM-et. Jelenitsuk meg a support  vectorokat es a decision boundaryt. Probajuk ki a soft-margin verziot is nem szeparalhato adatra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feladat\n",
    "Toltsd be az sklearn-bol a moons adatot. Plotold ki. Tanits nemlinearis SVM-et, probalj ki tobbfele kernel fuggvenyt. Plotold a support vectorokat es a decision boundaryt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. feladat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Egy adott $n$ mintaszamra generald le a $\\{(x_i, y_i)\\}^{n}_{i = 1}$ adatot, ahol $\\{x_{i}\\}$-k i.i.d mintak\n",
    "egyenletes eloszlasbol a $[0,1]$-en; legyen\n",
    "$y_i = f(x_i) + \\varepsilon_i$ , ahol $f(x) = c x sin(c x)$, es\n",
    "a zaj $\\{\\varepsilon_i\\}$ fuggetlen, standard normalis eloszlasbol szarmazo minta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n = 100, c = 16):\n",
    "    \"\"\"Generates the required sample of size n.\n",
    "\n",
    "    Keyword arguments:\n",
    "    n (int) -- sample size (default 100)\n",
    "    c (int) -- constant (default 16)\n",
    "    \n",
    "    Returns:\n",
    "        - x - numpy array of the data points\n",
    "        - y - numpy array of the function values\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = np.random.uniform(size = n)\n",
    "        y = c * x * np.sin(c * x) + np.random.standard_normal(size = n)\n",
    "        return x, y\n",
    "    except (ValueError, TypeError):\n",
    "        raise AssertionError(\"Cannot interpret n as a nonnegative integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanitsunk kernelizalt Linearis Regressziot. Roviden a lenyeg: \n",
    "Linearis Regresszio alapbol: ha az adat $d$ dimenzios es $n$ adatpont van, akkor legyen $\\Phi$ a kovetkezo matrix $n \\times d$-es matrix:\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\ldots & x_{1d}  \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & \\ldots & x_{nd}\n",
    "\\end{bmatrix}\n",
    "Szeretnenk egy olyan $\\theta$ vektort talalni, amire $\\Phi \\theta \\approx y$. Harom eset van, $n > d$, $n = d$ es $n < d$.\n",
    "Most az $n > d$ esettel foglalkozunk. Ekkor a rendszer tulhatarozott. Azt a megoldast keresem, amire \n",
    "$\\| y - \\Phi \\theta \\| ^2 _2$ minimalis. Ennek a neve Ordinary Least-Squares problema.\n",
    "Az optimalis megoldasa az OLS-nek a $\\theta = \\Phi^+$, azaz a Moore-Penrose fele pszeudo-inverz.\n",
    "\n",
    "Az egeszet megcsinalhatjuk kernelizalt valtozatban is, ha van $d$ db $f_i: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ alaku bazisfuggvenyem.\n",
    "Ekkor a $Phi$ matrix a kovetkezokeppen modosul:\n",
    "\\begin{bmatrix}\n",
    "f_1(x_1) & \\ldots & f_d(x_{1})  \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "f_1(x_{n}) & \\ldots & f_d(x_{n})\n",
    "\\end{bmatrix}\n",
    "\n",
    "A megoldas ugyanaz, mint elobb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feladat: \n",
    "\n",
    "tanitsunk kernelizalt linearis regressziot az iment generalt adatra. Legyenk a bazisfuggvenyek polinomok, vagyis  $f_i(x) = x^{i - 1}$.\n",
    "Eloszor implementaljuk le kezzel (hasznalhatjuk az np.linalg.pinv fuggvenyt), vagy ha nagyon nincs kedvunk, talaljuk ki, hogyan lehet az sklearn KernelRidge osztalyaval megcsinalni.\n",
    "Plotoljuk ki az illesztett fuggvenyt tobbfele $d$ es $n$ ertekekre is (de $n>d$ vegig maradjon igaz)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
